{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "Predicting Peak-Hour Transport Delays in Nairobi Using Machine Learning\n",
    "Rush Hour Roulette: Matatu Madness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Business Understanding\n",
    "Objective\n",
    "Build a supervised machine learning classification model to predict when and where overcrowding will occur at bus and matatu stops in Nairobi during peak hours and rainy seasons. This enables transport operators (SACCOs) to proactively deploy additional vehicles and reduce passenger queues before they form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Questions\n",
    "\n",
    "When are bus stops most likely to experience overcrowding? (Peak hours? Rainy days?)\n",
    "\n",
    "Which routes are most vulnerable to congestion-driven delays?\n",
    "\n",
    "What combination of factors (weather, time of day, route characteristics) best predicts overcrowding?\n",
    "\n",
    "Can we flag high-risk periods before queues form, rather than reacting after the fact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Impact\n",
    "For Transport Operators (SACCOs):\n",
    "\n",
    "Revenue Optimization: Deploy buses where/when demand is highest → capture more fares\n",
    "\n",
    "Fleet Efficiency: Reduce idle vehicles during off-peak times; maximize utilization during peaks\n",
    "\n",
    "Competitive Advantage: Operators with shorter wait times attract more customers\n",
    "\n",
    "# For Commuters:\n",
    "\n",
    "Reduced Wait Times: Fewer people left stranded during rush hour\n",
    "\n",
    "Improved Reliability: More predictable commute times\n",
    "\n",
    "Better Experience: Less frustration, safer boarding conditions\n",
    "\n",
    "# For Urban Planning:\n",
    "\n",
    "Data-Driven Decisions: Inform infrastructure investments (new routes, bus lanes)\n",
    "\n",
    "Traffic Management: Coordinate with traffic authorities during predicted congestion spikes\n",
    "\n",
    "Sustainability: Efficient public transport reduces private vehicle use\n",
    "\n",
    "# Crisis Management:\n",
    "\n",
    "Detect weather-triggered congestion early (e.g., heavy rain forecasts)\n",
    "\n",
    "Prepare contingency plans for major events (strikes, road closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "You're working with three main data sources:\n",
    "\n",
    "| Data Source                 | Purpose                                | Coverage                                      |\n",
    "|------------------------------|----------------------------------------|-----------------------------------------------|\n",
    "| **Weather Data (Open-Meteo API)** | Understand how rain/temperature affects demand | Hourly data for Nairobi, January 2024        |\n",
    "| **Traffic Data (Simulated)**       | Proxy for congestion levels           | Hourly travel times (35 min avg, 10–120 min range) |\n",
    "| **GTFS Transit Data (Static Feed)**| Route structure and stop locations    | 136 routes, 4,284 stops, 272 trips           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading Data and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# WEATHER DATA(Nairobi):\n",
    "\n",
    "\n",
    "SECTION 1: DOWNLOADING NAIROBI WEATHER DATA\n",
    "1. Rain increases matatu demand (people avoid walking/motorcycles)\n",
    "2. Rain slows down traffic → fewer buses reach stops per hour\n",
    "\n",
    "3. Temperature might affect demand (extreme heat/cold changes behavior)\n",
    "\n",
    "4. Weather is a proven predictor of transport congestion in urban areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 03:00:00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 04:00:00</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  temperature  precipitation\n",
       "0 2024-01-01 00:00:00         15.4            0.0\n",
       "1 2024-01-01 01:00:00         16.6            0.0\n",
       "2 2024-01-01 02:00:00         15.8            0.0\n",
       "3 2024-01-01 03:00:00         15.4            0.0\n",
       "4 2024-01-01 04:00:00         16.4            0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import the requests library to make HTTP API calls\n",
    "import requests\n",
    "\n",
    "# Define the API endpoint for Open-Meteo's historical weather archive\n",
    "# Open-Meteo provides free access to historical weather data worldwide\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Set up the API request parameters\n",
    "params = {\n",
    "    \"latitude\": -1.286389,   # Nairobi's latitude (south of equator, hence negative)\n",
    "    \"longitude\": 36.817223,  # Nairobi's longitude (east of prime meridian)\n",
    "    \"start_date\": \"2024-01-01\",  # Beginning of our study period\n",
    "    \"end_date\": \"2024-01-31\",    # End of study period (full month of January)\n",
    "    \"hourly\": [\"temperature_2m\", \"precipitation\"]  # Request hourly data for:\n",
    "                                                    # - temperature_2m: temp at 2 meters above ground\n",
    "                                                    # - precipitation: rainfall in mm per hour\n",
    "}\n",
    "\n",
    "# Make the GET request to the API with our parameters\n",
    "response = requests.get(url, params=params)\n",
    "# Parse the JSON response into a Python dictionary\n",
    "weather_json = response.json()\n",
    "\n",
    "\n",
    "# Convert the nested JSON structure into a pandas DataFrame\n",
    "# The API returns data in this structure: {\"hourly\": {\"time\": [...], \"temperature_2m\": [...], ...}}\n",
    "\n",
    "weather = pd.DataFrame({\n",
    "    \"time\": weather_json[\"hourly\"][\"time\"],              # Extract hourly timestamps\n",
    "    \"temperature\": weather_json[\"hourly\"][\"temperature_2m\"],  # Extract temperature readings\n",
    "    \"precipitation\": weather_json[\"hourly\"][\"precipitation\"]   # Extract rainfall amounts\n",
    "})\n",
    "\n",
    "\n",
    "# Convert the 'time' column from string format to proper datetime objects\n",
    "# This allows us to extract hour, day, month, etc. and merge with other time-based data\n",
    "weather[\"time\"] = pd.to_datetime(weather[\"time\"])\n",
    "\n",
    "# Display the first 5 rows to verify data loaded correctly\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SECTION 2: SIMULATED TRAFFIC/TRAVEL TIME DATA\n",
    "\n",
    "WHY WE'RE SIMULATING DATA:\n",
    "\n",
    " IDEAL: Real GPS data from matatu fleets showing actual travel times\n",
    "REALITY: This data is not publicly available or is expensive to access\n",
    "SOLUTION: Simulate realistic travel times as a PROXY for congestion levels\n",
    "\n",
    "In production, you would replace this with:\n",
    "\n",
    " Google Maps Traffic API (travel time estimates)\n",
    " \n",
    "GPS tracking data from matatu SACCOs\n",
    "\n",
    "Mobile network-based congestion data\n",
    "\n",
    "Traffic sensor data from Nairobi City Council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>avg_travel_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>39.967142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>33.617357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>41.476885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 03:00:00</td>\n",
       "      <td>50.230299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 04:00:00</td>\n",
       "      <td>32.658466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  avg_travel_time_min\n",
       "0 2024-01-01 00:00:00            39.967142\n",
       "1 2024-01-01 01:00:00            33.617357\n",
       "2 2024-01-01 02:00:00            41.476885\n",
       "3 2024-01-01 03:00:00            50.230299\n",
       "4 2024-01-01 04:00:00            32.658466"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "# This ensures we get the same \"random\" numbers every time we run the code\n",
    "# Important for scientific reproducibility and debugging\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Create a DataFrame with simulated travel times\n",
    "traffic = pd.DataFrame({\n",
    "    # Create timestamps matching our weather data (same 744 hours)\n",
    "    \"time\": pd.date_range(\"2024-01-01\", periods=len(weather), freq=\"H\"),\n",
    "    # Generate random travel times from a normal (Gaussian) distribution\n",
    "    # loc=35: Mean travel time is 35 minutes (typical matatu journey in Nairobi)\n",
    "    # scale=10: Standard deviation of 10 minutes (variability due to traffic)\n",
    "    # size=len(weather): Generate 744 values (one per hour)\n",
    "    \n",
    "    \"avg_travel_time_min\": np.random.normal(loc=35, scale=10, size=len(weather))\n",
    "})\n",
    "\n",
    "\n",
    "# INTERPRETATION OF NORMAL DISTRIBUTION:\n",
    "# - ~68% of values will be between 25-45 minutes (35 ± 10)\n",
    "# - ~95% of values will be between 15-55 minutes (35 ± 20)\n",
    "# - This creates realistic variability: some hours have light traffic, others heavy\n",
    "\n",
    "# Clip extreme values to realistic bounds\n",
    "# .clip(10, 120) ensures all values are between 10 and 120 minutes\n",
    "# WHY?\n",
    "# - Minimum 10 min: Even with zero traffic, matatus take some time\n",
    "# - Maximum 120 min: Extreme gridlock scenario (2 hours for routes that normally take 35 min)\n",
    "# Without clipping, normal distribution could generate negative or unrealistically high values\n",
    "traffic[\"avg_travel_time_min\"] = traffic[\"avg_travel_time_min\"].clip(10, 120)\n",
    "\n",
    "# Display first 5 rows to verify\n",
    "traffic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT TRAVEL TIME REPRESENTS:\n",
    " - LOW values (15-25 min): Off-peak hours, light traffic, fast journeys\n",
    "- MEDIUM values (30-40 min): Normal conditions\n",
    "- HIGH values (50+ min): Peak hour congestion, slow-moving traffic\n",
    "\n",
    "HIGH TRAVEL TIME = FEWER BUSES REACH STOPS PER HOUR = OVERCROWDING\n",
    "\n",
    "\n",
    "\n",
    "# KEY ASSUMPTIONS IN THIS SIMULATION:\n",
    " \n",
    " 1. Travel times follow a normal distribution (reasonable for aggregate data)\n",
    " 2. All routes have similar average times (simplification - in reality, varies by route)\n",
    " 3. Congestion is random (in reality, it follows time-of-day patterns)\n",
    "\n",
    "# LIMITATION: This simulated data won't capture real patterns like:\n",
    " - Morning rush (7-9 AM): HIGH travel times\n",
    " - Midday lull (11 AM - 3 PM): LOW travel times  \n",
    " - Evening rush (5-8 PM): VERY HIGH travel times\n",
    " - Weekend patterns: Different from weekdays\n",
    "\n",
    "TO IMPROVE: Add time-based patterns to the simulation:\n",
    "travel_time = base + peak_hour_penalty + random_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, we prepare the dataset by combining multiple data sources, engineering meaningful features, handling data quality issues, and creating our target variable. The main steps are:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extracting Temporal Features from Weather Data\n",
    "\n",
    "What we're doing:\n",
    "\n",
    "Converting text timestamps into Python datetime objects\n",
    "Creating two new columns: hour and dayofweek\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "These become critical features for our ML model\n",
    "Time patterns are the strongest predictors of transport demand\n",
    "Without these, the model can't learn that 7 AM behaves differently than 2 PM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'time' column is in proper datetime format\n",
    "# This allows us to extract specific time components (hour, day, etc.)\n",
    "weather[\"time\"] = pd.to_datetime(weather[\"time\"])\n",
    "\n",
    "# Extract hour of day (0-23)\n",
    "# WHY: Rush hour patterns differ by time\n",
    "# - 7-9 AM: Morning commute into CBD\n",
    "# - 5-8 PM: Evening commute out of CBD\n",
    "# - Midnight-5 AM: Very low demand\n",
    "\n",
    "weather[\"hour\"] = weather[\"time\"].dt.hour\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "# WHY: Weekday vs weekend patterns are dramatically different\n",
    "# - Mon-Fri: High peak-hour congestion (work commutes)\n",
    "# - Sat-Sun: Lower demand, different travel patterns\n",
    "\n",
    "weather[\"dayofweek\"] = weather[\"time\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Merging Weather and Traffic Data\n",
    "    \n",
    "What we're doing:\n",
    "\n",
    "Performing an inner join on the timestamp column\n",
    "Each row now contains: weather + traffic for one specific hour\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "We can now analyze: \"When it rained 2.5mm at 6 PM on a Monday, travel times were 55 minutes\"\n",
    "The model learns relationships between weather and congestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure traffic data also has proper datetime format\n",
    "traffic[\"time\"] = pd.to_datetime(traffic[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>avg_travel_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.617357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41.476885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 03:00:00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>50.230299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 04:00:00</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>32.658466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  temperature  precipitation  hour  dayofweek  \\\n",
       "0 2024-01-01 00:00:00         15.4            0.0     0          0   \n",
       "1 2024-01-01 01:00:00         16.6            0.0     1          0   \n",
       "2 2024-01-01 02:00:00         15.8            0.0     2          0   \n",
       "3 2024-01-01 03:00:00         15.4            0.0     3          0   \n",
       "4 2024-01-01 04:00:00         16.4            0.0     4          0   \n",
       "\n",
       "   avg_travel_time_min  \n",
       "0            39.967142  \n",
       "1            33.617357  \n",
       "2            41.476885  \n",
       "3            50.230299  \n",
       "4            32.658466  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge weather and traffic on the 'time' column\n",
    "# This combines weather conditions with congestion levels for each hour\n",
    "full = weather.merge(traffic, on=\"time\")\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 3: Loading GTFS Transit Data\n",
    " \n",
    "Opening a ZIP file without extracting it (efficient)\n",
    "Reading 4 different CSV files that define Nairobi's matatu system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stop_id           stop_name  stop_lat   stop_lon  location_type  \\\n",
      "0  0001RLW            Railways -1.290884  36.828242            0.0   \n",
      "1  0002KOJ                Koja -1.281230  36.822596            1.0   \n",
      "2  0003NGR               Ngara -1.274395  36.823806            1.0   \n",
      "3  0004ODN               Odeon -1.282769  36.825032            1.0   \n",
      "4  0005AMB  Kencom/Ambassadeur -1.285963  36.826048            1.0   \n",
      "\n",
      "  parent_station  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "      route_id agency_id route_short_name                 route_long_name  \\\n",
      "0  10000107D11       UON             107D                     Ruaka-Ruiru   \n",
      "1  10000114011       UON             114R     Ngara-Rwaka-Ndenderu-Limuru   \n",
      "2  10000116011       UON              116        Koja-Ngara-Banana-Limuru   \n",
      "3  10100011A11       UON              11A        Odeon-Aga Khan-Highridge   \n",
      "4  10200010811       UON              108  UN-New Muthaiga-Gachie-Gichagi   \n",
      "\n",
      "   route_type  \n",
      "0           3  \n",
      "1           3  \n",
      "2           3  \n",
      "3           3  \n",
      "4           3  \n",
      "(4284, 6)\n",
      "(136, 5)\n",
      "(272, 6)\n",
      "(7533, 5)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the GTFS ZIP file (standard transit data format)\n",
    "\n",
    "gtfs_zip_path = \"GTFS_FEED_2019.zip\"\n",
    "\n",
    "# Extract and load multiple CSV files from the ZIP archive\n",
    "with zipfile.ZipFile(gtfs_zip_path, 'r') as z:\n",
    "    stops = pd.read_csv(z.open('stops.txt'))        # Bus stop locations\n",
    "    routes = pd.read_csv(z.open('routes.txt'))      # Route definitions\n",
    "    trips = pd.read_csv(z.open('trips.txt'))        # Individual trips\n",
    "    stop_times = pd.read_csv(z.open('stop_times.txt'))  # Trip schedules\n",
    "\n",
    "# Verify data loaded correctly\n",
    "print(stops.head())\n",
    "print(routes.head())\n",
    "\n",
    "# Quick sanity check\n",
    "print(stops.shape)\n",
    "print(routes.shape)\n",
    "print(trips.shape)\n",
    "print(stop_times.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agency.txt', 'calendar.txt', 'calendar_dates.txt', 'feed_info.txt', 'frequencies.txt', 'routes.txt', 'shapes.txt', 'stop_times.txt', 'stops.txt', 'trips.txt']\n"
     ]
    }
   ],
   "source": [
    "#Confirm What GTFS Tables You Have\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"GTFS_FEED_2019.zip\", \"r\") as z:\n",
    "    print(z.namelist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Understanding Route-Stop Relationships\n",
    "\n",
    "What we're doing:\n",
    "\n",
    "Performing two consecutive merges to connect three tables\n",
    "Creating a master table: \"Which stops are on which routes?\"\n",
    "\n",
    "Why we need this:\n",
    "\n",
    "Different routes have different characteristics (length, number of stops, distance from CBD)\n",
    "These characteristics affect overcrowding risk\n",
    "Example: A 40km route is more vulnerable to traffic delays than a 5km route\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10300106011</td>\n",
       "      <td>0110BAA</td>\n",
       "      <td>Banana Terminus</td>\n",
       "      <td>-1.174235</td>\n",
       "      <td>36.758765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000116011</td>\n",
       "      <td>0110BAA</td>\n",
       "      <td>Banana Terminus</td>\n",
       "      <td>-1.174235</td>\n",
       "      <td>36.758765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10300106011</td>\n",
       "      <td>0110BNI</td>\n",
       "      <td>Kobil banana</td>\n",
       "      <td>-1.174564</td>\n",
       "      <td>36.758993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10300106011</td>\n",
       "      <td>0110UMK</td>\n",
       "      <td>Mkulima supermarket</td>\n",
       "      <td>-1.175079</td>\n",
       "      <td>36.759251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10300106011</td>\n",
       "      <td>0110AEA</td>\n",
       "      <td>Shell Banana</td>\n",
       "      <td>-1.175848</td>\n",
       "      <td>36.759530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      route_id  stop_id            stop_name  stop_lat   stop_lon\n",
       "0  10300106011  0110BAA      Banana Terminus -1.174235  36.758765\n",
       "1  10000116011  0110BAA      Banana Terminus -1.174235  36.758765\n",
       "2  10300106011  0110BNI         Kobil banana -1.174564  36.758993\n",
       "3  10300106011  0110UMK  Mkulima supermarket -1.175079  36.759251\n",
       "4  10300106011  0110AEA         Shell Banana -1.175848  36.759530"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link stop_times → trips → stops to understand which stops belong to which routes\n",
    "route_stops = (\n",
    "    stop_times\n",
    "    .merge(trips[[\"trip_id\",\"route_id\"]], on=\"trip_id\")  # Add route info to stops\n",
    "    .merge(stops[[\"stop_id\",\"stop_name\",\"stop_lat\",\"stop_lon\"]], on=\"stop_id\")  # Add location info\n",
    "    [[\"route_id\",\"stop_id\",\"stop_name\",\"stop_lat\",\"stop_lon\"]]  # Keep only needed columns\n",
    "    .drop_duplicates()  # Remove duplicate stop-route combinations\n",
    ")\n",
    "route_stops.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Engineering Route Features\n",
    "\n",
    "Feature 1: Number of Stops per Route\n",
    "\n",
    "What we're doing:\n",
    "\n",
    "Grouping by route, counting how many different stops it serves\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "More stops = longer journey = more exposure to traffic delays\n",
    "Routes with 150 stops vs 35 stops behave very differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique stops for each route\n",
    "route_stop_count = (\n",
    "    route_stops.groupby(\"route_id\")[\"stop_id\"]\n",
    "    .nunique()  # Count distinct stops (not total occurrences)\n",
    "    .reset_index(name=\"num_stops_on_route\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Haversine Formula:\n",
    "\n",
    "What this calculates:\n",
    "\n",
    "Straight-line distance between two GPS coordinates on Earth's curved surface Not exact road distance, but close enough for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "\n",
    "    lat1, lon1, lat2, lon2 = map(\n",
    "        math.radians, [lat1, lon1, lat2, lon2]\n",
    "    )\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = (\n",
    "        math.sin(dlat/2)**2\n",
    "        + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    )\n",
    "\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 2: Route Length (CORRECTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Build a trip table with only required columns\n",
    "trip_stops = (\n",
    "    stop_times\n",
    "    .merge(trips[['trip_id','route_id']], on='trip_id')\n",
    "    .merge(stops[['stop_id','stop_lat','stop_lon']], on='stop_id')\n",
    "    [['route_id','trip_id','stop_sequence','stop_lat','stop_lon']]\n",
    ")\n",
    "\n",
    "# STEP 2: Select ONE representative trip per route\n",
    "# Pick the longest trip (most stops = most complete path)\n",
    "trip_lengths = (\n",
    "    trip_stops.groupby('trip_id')['stop_sequence']\n",
    "    .count()\n",
    "    .reset_index(name='n_stops')\n",
    ")\n",
    "\n",
    "longest_trips = (\n",
    "    trip_lengths\n",
    "    .sort_values('n_stops', ascending=False)  # Sort by trip length\n",
    "    .merge(trips[['trip_id','route_id']], on='trip_id')\n",
    "    .drop_duplicates('route_id')  # Keep only the longest trip per route\n",
    ")\n",
    "\n",
    "# STEP 3: Merge back and keep stops in CORRECT ORDER\n",
    "route_paths = (\n",
    "    longest_trips[['route_id','trip_id']]\n",
    "    .merge(trip_stops, on=['route_id','trip_id'])\n",
    "    .sort_values(['route_id','stop_sequence'])  # KEY: Sort by stop_sequence!\n",
    ")\n",
    "\n",
    "# STEP 4: Recalculate distance correctly\n",
    "route_lengths = []\n",
    "\n",
    "for rid, g in route_paths.groupby('route_id'):\n",
    "    dist = 0\n",
    "    g = g.reset_index(drop=True)\n",
    "    # Now we calculate distance between CONSECUTIVE stops in trip order\n",
    "    for i in range(len(g)-1):\n",
    "        dist += haversine(\n",
    "            g.loc[i,'stop_lat'], g.loc[i,'stop_lon'],\n",
    "            g.loc[i+1,'stop_lat'], g.loc[i+1,'stop_lon']\n",
    "        )\n",
    "    route_lengths.append([rid, dist])\n",
    "\n",
    "route_length_df = pd.DataFrame(route_lengths, columns=['route_id','route_length_km'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we fixed:\n",
    "\n",
    "Used stop_sequence field (1, 2, 3, ...) to order stops correctly\n",
    "Selected the longest trip per route as a representative path\n",
    "Calculated distances between consecutive stops only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Route Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_features = (\n",
    "    route_stop_count\n",
    "    .merge(route_length_df, on='route_id', how='inner')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    136.000000\n",
       "mean      12.294688\n",
       "std        8.118641\n",
       "min        1.208775\n",
       "25%        6.581010\n",
       "50%       10.148127\n",
       "75%       15.862352\n",
       "max       41.547099\n",
       "Name: route_length_km, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check results:\n",
    "route_features['route_length_km'].describe()\n",
    "\n",
    "#Mean: 12.3 km (realistic for Nairobi matatu routes ✓)\n",
    "#Min: 1.2 km (short inner-city routes)\n",
    "#Max: 41.5 km (long routes to satellite towns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM: This calculates distance between ALL stop pairs, not just consecutive ones\n",
    "#What #went wrong:\n",
    "\n",
    "#We sorted stops alphabetically by stop_id (e.g., \"0001\", \"0002\", \"0003\")\n",
    "#But the actual route path doesn't follow alphabetical order!\n",
    "#Example: Route might go: Stop_0005 → Stop_0002 → Stop_0010\n",
    "#This gave absurd results: 593km, 1019km (longer than Kenya itself!)\n",
    "\n",
    "#from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "#def haversine(lat1, lon1, lat2, lon2):\n",
    "   # R = 6371\n",
    "    #dlat = radians(lat2-lat1)\n",
    "    #dlon = radians(lon2-lon1)\n",
    "    #a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "    #return 2*R*atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "#route_lengths = []\n",
    "\n",
    "#for rid, group in route_stops.groupby(\"route_id\"):\n",
    "    #group = group.sort_values(\"stop_id\")\n",
    "    #dist = 0\n",
    "    #for i in range(len(group)-1):\n",
    "        #dist += haversine(group.iloc[i].stop_lat,\n",
    "                         # group.iloc[i].stop_lon,\n",
    "                          #group.iloc[i+1].stop_lat,\n",
    "                          #group.iloc[i+1].stop_lon)\n",
    "    #route_lengths.append([rid, dist])\n",
    "    \n",
    "\n",
    "#route_length_df = pd.DataFrame(route_lengths, columns=[\"route_id\",\"route_length_km\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 3: Average Distance to CBD\n",
    "\n",
    "What we're doing:\n",
    "\n",
    "Measuring how far each route operates from the city center\n",
    "Taking the average distance across all stops\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "CBD-bound routes (inbound): Heavy evening congestion (5-8 PM)\n",
    "Suburb-bound routes (outbound): Heavy morning congestion (7-9 AM)\n",
    "Distance from CBD is a proxy for whether route faces inbound or outbound traffic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Nairobi CBD coordinates (city center)\n",
    "CBD_LAT = -1.286389\n",
    "CBD_LON = 36.817223\n",
    "\n",
    "# Calculate average distance of all route stops from CBD\n",
    "route_cbd = []\n",
    "\n",
    "for rid, group in route_stops.groupby(\"route_id\"):\n",
    "    # Calculate distance from CBD to each stop on this route\n",
    "    d = group.apply(\n",
    "        lambda r: haversine(CBD_LAT, CBD_LON, r.stop_lat, r.stop_lon),\n",
    "        axis=1\n",
    "    )\n",
    "    # Take the mean distance (average across all stops)\n",
    "    route_cbd.append([rid, d.mean()])\n",
    "\n",
    "route_cbd_df = pd.DataFrame(route_cbd, columns=[\"route_id\",\"avg_distance_to_CBD_km\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging All Route Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>route_short_name</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>num_stops_on_route</th>\n",
       "      <th>route_length_km</th>\n",
       "      <th>avg_distance_to_CBD_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000107D11</td>\n",
       "      <td>107D</td>\n",
       "      <td>Ruaka-Ruiru</td>\n",
       "      <td>68</td>\n",
       "      <td>25.237749</td>\n",
       "      <td>13.490841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000114011</td>\n",
       "      <td>114R</td>\n",
       "      <td>Ngara-Rwaka-Ndenderu-Limuru</td>\n",
       "      <td>149</td>\n",
       "      <td>30.902494</td>\n",
       "      <td>11.649784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000116011</td>\n",
       "      <td>116</td>\n",
       "      <td>Koja-Ngara-Banana-Limuru</td>\n",
       "      <td>94</td>\n",
       "      <td>30.931164</td>\n",
       "      <td>11.254005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10100011A11</td>\n",
       "      <td>11A</td>\n",
       "      <td>Odeon-Aga Khan-Highridge</td>\n",
       "      <td>35</td>\n",
       "      <td>4.436434</td>\n",
       "      <td>2.503058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10200010811</td>\n",
       "      <td>108</td>\n",
       "      <td>UN-New Muthaiga-Gachie-Gichagi</td>\n",
       "      <td>44</td>\n",
       "      <td>6.012415</td>\n",
       "      <td>7.995713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      route_id route_short_name                 route_long_name  \\\n",
       "0  10000107D11             107D                     Ruaka-Ruiru   \n",
       "1  10000114011             114R     Ngara-Rwaka-Ndenderu-Limuru   \n",
       "2  10000116011              116        Koja-Ngara-Banana-Limuru   \n",
       "3  10100011A11              11A        Odeon-Aga Khan-Highridge   \n",
       "4  10200010811              108  UN-New Muthaiga-Gachie-Gichagi   \n",
       "\n",
       "   num_stops_on_route  route_length_km  avg_distance_to_CBD_km  \n",
       "0                  68        25.237749               13.490841  \n",
       "1                 149        30.902494               11.649784  \n",
       "2                  94        30.931164               11.254005  \n",
       "3                  35         4.436434                2.503058  \n",
       "4                  44         6.012415                7.995713  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine route metadata with engineered features\n",
    "route_features = (\n",
    "    routes[['route_id','route_short_name','route_long_name']]\n",
    "    .merge(route_stop_count, on='route_id')       # Add: number of stops\n",
    "    .merge(route_length_df, on='route_id')        # Add: route length\n",
    "    .merge(route_cbd_df, on='route_id')           # Add: CBD distance\n",
    ")\n",
    "\n",
    "route_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Creating the Machine Learning Dataset (Cartesian Join)\n",
    "    \n",
    " This is called a Cartesian product (cross join):\n",
    "\n",
    "Every hour is paired with every route\n",
    "Allows model to learn route-specific patterns at different times\n",
    "\n",
    "Example rows:\n",
    "\n",
    "Row 1: Route 107D at midnight on Jan 1\n",
    "Row 2: Route 114R at midnight on Jan 1\n",
    "Row 3: Route 116 at midnight on Jan 1\n",
    "...\n",
    "Row 137: Route 107D at 1 AM on Jan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy column to both datasets (all values = 1)\n",
    "route_features[\"key\"] = 1\n",
    "full[\"key\"] = 1\n",
    "\n",
    "# Merge on the dummy column (creates every possible combination)\n",
    "ml = full.merge(route_features, on=\"key\").drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Creating the Target Variable\n",
    "\n",
    "Our target variable delay_overcrowding identifies problematic hours when bus stops are likely to be overcrowded. We define this using two conditions:\n",
    "\n",
    "    \n",
    " Why this definition?\n",
    "\n",
    "Travel time threshold (75th percentile):\n",
    "\n",
    "Captures the slowest 25% of hours\n",
    "These are when traffic congestion is severe\n",
    "Fewer vehicles reach stops → queues build up\n",
    "\n",
    "\n",
    "Rain threshold (>0.5 mm):\n",
    "\n",
    "Moderate to heavy rainfall\n",
    "Increases demand (people avoid walking/boda bodas)\n",
    "Slows traffic (visibility, caution)\n",
    "Double impact: higher demand + lower supply\n",
    "\n",
    "\n",
    "Logical OR (|):\n",
    "\n",
    "Overcrowding can happen from traffic OR rain OR both\n",
    "If either condition is met, label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution (before route expansion):\n",
      "0    0.721774\n",
      "1    0.278226\n",
      "Name: delay_overcrowding, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Create target variable in 'full' BEFORE merging with routes\n",
    "# This ensures we calculate the threshold on actual hourly data, not duplicated route-hour combinations\n",
    "\n",
    "# Step 1: Calculate the 75th percentile threshold from hourly travel times\n",
    "threshold = full[\"avg_travel_time_min\"].quantile(0.75)\n",
    "\n",
    "# Step 2: Create binary target variable in the 'full' DataFrame\n",
    "# 1 = Delayed/Overcrowded if EITHER:\n",
    "#     - Travel time > 75th percentile (top 25% slowest hours), OR\n",
    "#     - Heavy rain (> 0.5 mm/hour)\n",
    "# 0 = Normal otherwise\n",
    "full[\"delay_overcrowding\"] = (\n",
    "    (full[\"avg_travel_time_min\"] > threshold) |  # Logical OR operator\n",
    "    (full[\"precipitation\"] > 0.5)\n",
    ").astype(int)  # Convert True/False to 1/0\n",
    "\n",
    "# Step 3: Check the class distribution before merging\n",
    "print(\"Target variable distribution (before route expansion):\")\n",
    "print(full[\"delay_overcrowding\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "72% Class 0 (normal hours) - 536 out of 744 hours\n",
    "28% Class 1 (overcrowded hours) - 208 out of 744 hours\n",
    "Slightly imbalanced but acceptable for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 8: Creating the Final ML Dataset (Route-Hour Panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (101184, 13)\n",
      "Expected rows: 744 hours × 136 routes = 101184\n"
     ]
    }
   ],
   "source": [
    "# Add dummy column to both DataFrames for Cartesian product\n",
    "route_features[\"key\"] = 1\n",
    "full[\"key\"] = 1\n",
    "\n",
    "# Perform cross join: Every hour paired with every route\n",
    "ml = full.merge(route_features, on=\"key\").drop(\"key\", axis=1)\n",
    "\n",
    "print(f\"Final dataset shape: {ml.shape}\")\n",
    "print(f\"Expected rows: {len(full)} hours × {len(route_features)} routes = {len(full) * len(route_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preview the merged dataset:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>avg_travel_time_min</th>\n",
       "      <th>delay_overcrowding</th>\n",
       "      <th>route_id</th>\n",
       "      <th>route_short_name</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>num_stops_on_route</th>\n",
       "      <th>route_length_km</th>\n",
       "      <th>avg_distance_to_CBD_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "      <td>0</td>\n",
       "      <td>10000107D11</td>\n",
       "      <td>107D</td>\n",
       "      <td>Ruaka-Ruiru</td>\n",
       "      <td>68</td>\n",
       "      <td>25.237749</td>\n",
       "      <td>13.490841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "      <td>0</td>\n",
       "      <td>10000114011</td>\n",
       "      <td>114R</td>\n",
       "      <td>Ngara-Rwaka-Ndenderu-Limuru</td>\n",
       "      <td>149</td>\n",
       "      <td>30.902494</td>\n",
       "      <td>11.649784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "      <td>0</td>\n",
       "      <td>10000116011</td>\n",
       "      <td>116</td>\n",
       "      <td>Koja-Ngara-Banana-Limuru</td>\n",
       "      <td>94</td>\n",
       "      <td>30.931164</td>\n",
       "      <td>11.254005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "      <td>0</td>\n",
       "      <td>10100011A11</td>\n",
       "      <td>11A</td>\n",
       "      <td>Odeon-Aga Khan-Highridge</td>\n",
       "      <td>35</td>\n",
       "      <td>4.436434</td>\n",
       "      <td>2.503058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.967142</td>\n",
       "      <td>0</td>\n",
       "      <td>10200010811</td>\n",
       "      <td>108</td>\n",
       "      <td>UN-New Muthaiga-Gachie-Gichagi</td>\n",
       "      <td>44</td>\n",
       "      <td>6.012415</td>\n",
       "      <td>7.995713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  temperature  precipitation  hour  dayofweek  \\\n",
       "0 2024-01-01         15.4            0.0     0          0   \n",
       "1 2024-01-01         15.4            0.0     0          0   \n",
       "2 2024-01-01         15.4            0.0     0          0   \n",
       "3 2024-01-01         15.4            0.0     0          0   \n",
       "4 2024-01-01         15.4            0.0     0          0   \n",
       "\n",
       "   avg_travel_time_min  delay_overcrowding     route_id route_short_name  \\\n",
       "0            39.967142                   0  10000107D11             107D   \n",
       "1            39.967142                   0  10000114011             114R   \n",
       "2            39.967142                   0  10000116011              116   \n",
       "3            39.967142                   0  10100011A11              11A   \n",
       "4            39.967142                   0  10200010811              108   \n",
       "\n",
       "                  route_long_name  num_stops_on_route  route_length_km  \\\n",
       "0                     Ruaka-Ruiru                  68        25.237749   \n",
       "1     Ngara-Rwaka-Ndenderu-Limuru                 149        30.902494   \n",
       "2        Koja-Ngara-Banana-Limuru                  94        30.931164   \n",
       "3        Odeon-Aga Khan-Highridge                  35         4.436434   \n",
       "4  UN-New Muthaiga-Gachie-Gichagi                  44         6.012415   \n",
       "\n",
       "   avg_distance_to_CBD_km  \n",
       "0               13.490841  \n",
       "1               11.649784  \n",
       "2               11.254005  \n",
       "3                2.503058  \n",
       "4                7.995713  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 9: Final Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (101184, 13)\n",
      "Number of rows: 101,184\n",
      "Number of columns: 13\n",
      "\n",
      "Target variable distribution:\n",
      "0    0.721774\n",
      "1    0.278226\n",
      "Name: delay_overcrowding, dtype: float64\n",
      "\n",
      "Missing values per column:\n",
      "time                      0\n",
      "temperature               0\n",
      "precipitation             0\n",
      "hour                      0\n",
      "dayofweek                 0\n",
      "avg_travel_time_min       0\n",
      "delay_overcrowding        0\n",
      "route_id                  0\n",
      "route_short_name          0\n",
      "route_long_name           0\n",
      "num_stops_on_route        0\n",
      "route_length_km           0\n",
      "avg_distance_to_CBD_km    0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "time                      datetime64[ns]\n",
      "temperature                      float64\n",
      "precipitation                    float64\n",
      "hour                               int64\n",
      "dayofweek                          int64\n",
      "avg_travel_time_min              float64\n",
      "delay_overcrowding                 int32\n",
      "route_id                          object\n",
      "route_short_name                  object\n",
      "route_long_name                   object\n",
      "num_stops_on_route                 int64\n",
      "route_length_km                  float64\n",
      "avg_distance_to_CBD_km           float64\n",
      "dtype: object\n",
      "\n",
      "Numerical feature summary:\n",
      "         temperature  precipitation  avg_travel_time_min  num_stops_on_route  \\\n",
      "count  101184.000000  101184.000000        101184.000000       101184.000000   \n",
      "mean       19.777957       0.111425            34.873500           53.852941   \n",
      "std         3.742279       0.539533             9.802024           28.603548   \n",
      "min        13.600000       0.000000            10.000000            6.000000   \n",
      "25%        16.500000       0.000000            27.996926           34.000000   \n",
      "50%        18.600000       0.000000            35.051786           50.000000   \n",
      "75%        23.100000       0.000000            41.286663           69.250000   \n",
      "max        27.800000       9.100000            73.527315          149.000000   \n",
      "\n",
      "       route_length_km  avg_distance_to_CBD_km  \n",
      "count    101184.000000           101184.000000  \n",
      "mean         12.294688                7.948356  \n",
      "std           8.088778                4.497515  \n",
      "min           1.208775                1.195421  \n",
      "25%           6.581010                4.773927  \n",
      "50%          10.148127                7.527454  \n",
      "75%          15.862352                9.705102  \n",
      "max          41.547099               25.801621  \n"
     ]
    }
   ],
   "source": [
    "# Check 1: Dataset dimensions\n",
    "print(f\"Dataset shape: {ml.shape}\")\n",
    "print(f\"Number of rows: {ml.shape[0]:,}\")\n",
    "print(f\"Number of columns: {ml.shape[1]}\")\n",
    "\n",
    "# Check 2: Target variable distribution (should match 'full' percentages)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(ml[\"delay_overcrowding\"].value_counts(normalize=True))\n",
    "\n",
    "# Check 3: Missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(ml.isna().sum())\n",
    "\n",
    "# Check 4: Data types\n",
    "print(\"\\nData types:\")\n",
    "print(ml.dtypes)\n",
    "\n",
    "# Check 5: Summary statistics for numerical features\n",
    "print(\"\\nNumerical feature summary:\")\n",
    "print(ml[['temperature', 'precipitation', 'avg_travel_time_min', \n",
    "          'num_stops_on_route', 'route_length_km', 'avg_distance_to_CBD_km']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 .Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3  Baseline Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 — Evaluate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 — Random Forest Model (Stronger Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 6 — Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest achieved 100% accuracy. However, this performance is artificially inflated due to data leakage. The target variable was partially defined using average travel time, which was also included as an input feature. This means the model was indirectly given information about the label during training, making the prediction trivial. When evaluated using Logistic Regression — a less flexible model — performance remained high but realistic (97% accuracy, 94% recall), suggesting the features are still informative even without leakage.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 — Feature Importance (Very Important for Your Write-Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congestion level drives overcrowding the most (logical)\n",
    "* Rainfall meaningfully increases overcrowding risk\n",
    "* Temperature & time of day matter a little\n",
    "* Route structure has very small effect (in this dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Leakage may have happened because\n",
    "We created the target using avg_travel_time_min, and then we also used avg_travel_time_min as a feature.\n",
    "So your model is learning the rule we created. It is basically being handed the answer.\n",
    "That’s why Random Forest = 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a no leakage model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 — Define a SAFE feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_features = [\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"num_stops_on_route\",\n",
    "    \"route_length_km\",\n",
    "    \"avg_distance_to_CBD_km\"\n",
    "]\n",
    "\n",
    "X = ml[safe_features]\n",
    "y = ml[\"delay_overcrowding\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 — Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 — Logistic Regression (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 — Random Forest (now leakage-free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 — Feature Importance (interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": safe_features,\n",
    "    \"importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the leaking travel-time variable, Logistic Regression achieved 76% accuracy but only 15% recall for overcrowding, indicating it was overly conservative. Random Forest improved recall to 39% while maintaining 100% precision, meaning every overcrowding alert was correct but many events were still missed. This reflects the difficulty of predicting overcrowding based only on pre-event factors such as weather, time-of-day, and route structure. Precision-recall tuning was identified as a practical method to improve detection performance depending on operational priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We tune the decision threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Random Forest Probabilities\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prob = rf.predict_proba(X_test)[:,1]   # probability of class 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the Threshold\n",
    "\n",
    "#Default is 0.5. Let’s try 0.35 first (common sweet spot):\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "threshold = 0.35\n",
    "y_pred_tuned = (y_prob >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "print(confusion_matrix(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying Multiple Thresholds\n",
    "for t in [0.5, 0.45, 0.40, 0.35, 0.30, 0.25]:\n",
    "    y_pred = (y_prob >= t).astype(int)\n",
    "    print(f\"\\nTHRESHOLD = {t}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the default 0.5 threshold the Random Forest model rarely raised false alarms, but it missed many overcrowding events. Since the cost of missing real overcrowding is higher than the cost of a false alert, the probability threshold was reduced. At a tuned threshold of 0.35, recall improved from 39% to 81% while precision remained high at 92%. This represents a practical balance between sensitivity and reliability for real-world transport operations.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_prob_xgb = xgb.predict_proba(X_test)[:,1]\n",
    "y_pred_xgb = (y_prob_xgb >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Threshold for XGBoost Too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.35\n",
    "y_pred_xgb_tuned = (y_prob_xgb >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb_tuned))\n",
    "print(confusion_matrix(y_test, y_pred_xgb_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost achieved 97% overall accuracy at the default 0.5 probability threshold, with 98% precision and 90% recall for overcrowding events. Since the operational objective prioritizes detecting overcrowding, the threshold was reduced to 0.35. This increased recall to 98% while maintaining 91% precision and 97% accuracy, meaning almost all overcrowding events were detected and false alarms remained low. Threshold tuning therefore provided a practical means to balance sensitivity and reliability in a transport planning context.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_features = [\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"num_stops_on_route\",\n",
    "    \"route_length_km\",\n",
    "    \"avg_distance_to_CBD_km\"\n",
    "]\n",
    "\n",
    "X = ml[safe_features]\n",
    "y = ml[\"delay_overcrowding\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cat.fit(X_train, y_train)\n",
    "\n",
    "y_prob_cat = cat.predict_proba(X_test)[:,1]\n",
    "y_pred_cat = (y_prob_cat >= 0.5).astype(int)\n",
    "\n",
    "print(\"CATBOOST — THRESHOLD 0.50\")\n",
    "print(classification_report(y_test, y_pred_cat))\n",
    "print(confusion_matrix(y_test, y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.35\n",
    "y_pred_cat_tuned = (y_prob_cat >= threshold).astype(int)\n",
    "\n",
    "print(\"CATBOOST — THRESHOLD 0.35 (TUNED)\")\n",
    "print(classification_report(y_test, y_pred_cat_tuned))\n",
    "print(confusion_matrix(y_test, y_pred_cat_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost achieved the strongest performance across all models evaluated. At the default probability threshold (0.5), CatBoost reached 98% accuracy and 94% recall for overcrowding events. Since the primary objective is to detect overcrowding early, the probability threshold was reduced to 0.35. This increased recall to 100% while precision remained high at 94%. This means the model successfully detected virtually every overcrowding period while keeping false alarms relatively low, making it highly suitable for operational deployment.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def model_summary_row(name, threshold, y_true, y_pred, explanation):\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', pos_label=1\n",
    "    )\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Threshold\": threshold,\n",
    "        \"Accuracy\": round(acc, 3),\n",
    "        \"Precision (Class 1)\": round(precision, 3),\n",
    "        \"Recall (Class 1)\": round(recall, 3),\n",
    "        \"F1 (Class 1)\": round(f1, 3),\n",
    "        \"Explanation\": explanation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"Logistic Regression\",\n",
    "    0.50,\n",
    "    y_test,\n",
    "    y_pred_lr,\n",
    "    \"Linear baseline — high accuracy but misses many overcrowding events.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"Random Forest\",\n",
    "    0.50,\n",
    "    y_test,\n",
    "    y_pred_rf,\n",
    "    \"Very conservative — almost no false alarms but low recall.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"Random Forest (Tuned)\",\n",
    "    0.35,\n",
    "    y_test,\n",
    "    y_pred_tuned,   # RF tuned preds\n",
    "    \"Lower threshold increased recall while keeping precision high.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"XGBoost\",\n",
    "    0.50,\n",
    "    y_test,\n",
    "    y_pred_xgb,\n",
    "    \"Strong performance — good balance between recall and precision.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"XGBoost (Tuned)\",\n",
    "    0.35,\n",
    "    y_test,\n",
    "    y_pred_xgb_tuned,\n",
    "    \"Threshold tuning prioritised detecting overcrowding — recall rose to ~98%.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"CatBoost\",\n",
    "    0.50,\n",
    "    y_test,\n",
    "    y_pred_cat,\n",
    "    \"Best default model — high recall and precision out-of-the-box.\"\n",
    "))\n",
    "\n",
    "rows.append(model_summary_row(\n",
    "    \"CatBoost (Tuned)\",\n",
    "    0.35,\n",
    "    y_test,\n",
    "    y_pred_cat_tuned,\n",
    "    \"Final model — detects almost all overcrowding events with high reliability.\"\n",
    "))\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(rows)\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project set out to build a supervised machine learning system capable of predicting overcrowding and delay risk on Nairobi’s public transport network using GTFS route data, weather records, and congestion proxies. A full end-to-end workflow was implemented — from data ingestion and feature engineering, through careful target definition and model training, to rigorous evaluation and threshold optimisation.\n",
    "\n",
    "A key learning outcome was the identification and resolution of data leakage when travel-time was originally included both in the target construction and the feature set. Removing the leaking variable and rebuilding the models resulted in more realistic performance estimates and strengthened the scientific integrity of the study. Multiple model families were then evaluated, including Logistic Regression, Random Forest, XGBoost and CatBoost. While Logistic Regression provided a useful baseline, gradient boosting models consistently delivered superior performance.\n",
    "\n",
    "CatBoost emerged as the strongest model. At a tuned probability threshold of 0.35, the model achieved 98% overall accuracy, 94% precision and 100% recall for overcrowding events, meaning that virtually every high-risk period was detected while false alarms remained limited. Probability-threshold tuning proved to be an effective way to balance recall and precision depending on operational priorities. In this context, recall was prioritised because the cost of missing an overcrowding period is typically higher than the cost of a false alert.\n",
    "\n",
    "These results demonstrate that overcrowding in Nairobi’s matatu network is highly predictable using pre-event indicators such as weather, time of day and route structure, even without relying directly on real-time demand data. This provides a strong foundation for the development of early-warning tools that could support operators, regulators and city planners by informing proactive fleet allocation, service adjustments and passenger communication.\n",
    "\n",
    "However, the work also has important limitations. The target variable was derived from indirect congestion proxies rather than true passenger counts, weather data was city-level rather than route-level, and the system has not yet been validated in a live operational setting. Future work should incorporate real passenger load data, high-resolution route-specific weather feeds, live traffic telemetry and model calibration with operational experts. Further research into fairness across routes and socioeconomic areas would also be valuable.\n",
    "\n",
    "Overall, the project successfully demonstrates that data-driven predictive analytics can play a meaningful role in improving public transport resilience and passenger experience in emerging-market cities such as Nairobi. The final model is accurate, explainable and operationally relevant, and the process reflects strong machine-learning practice — from leakage control to model comparison and threshold tuning. This work therefore represents both a technically robust and socially impactful application of supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First — make sure we have your final model + features\n",
    "\n",
    "#We assume you already have:\n",
    "\n",
    "safe_features = [\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"num_stops_on_route\",\n",
    "    \"route_length_km\",\n",
    "    \"avg_distance_to_CBD_km\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dataset for “future hours”\n",
    "\n",
    "#This version creates a Route × Hour panel for the next day using your weather table (or you can plug in a future forecast later easily).\n",
    "\n",
    "# Copy only needed columns\n",
    "future = weather.copy()[[\"time\",\"temperature\",\"precipitation\"]]\n",
    "\n",
    "future[\"time\"] = pd.to_datetime(future[\"time\"])\n",
    "future[\"hour\"] = future[\"time\"].dt.hour\n",
    "future[\"dayofweek\"] = future[\"time\"].dt.dayofweek\n",
    "\n",
    "# --- Create route × hour combinations ---\n",
    "route_features[\"key\"] = 1\n",
    "future[\"key\"] = 1\n",
    "\n",
    "future_panel = future.merge(route_features, on=\"key\").drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict overcrowding probability\n",
    "X_future = future_panel[safe_features]\n",
    "\n",
    "future_panel[\"prob_overcrowding\"] = cat.predict_proba(X_future)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a simple RISK label\n",
    "\n",
    "#(Using the same tuned threshold = 0.35)\n",
    "\n",
    "def risk_label(p):\n",
    "    if p >= 0.75:\n",
    "        return \"HIGH RISK\"\n",
    "    elif p >= 0.35:\n",
    "        return \"MEDIUM RISK\"\n",
    "    else:\n",
    "        return \"LOW RISK\"\n",
    "\n",
    "future_panel[\"Risk Level\"] = future_panel[\"prob_overcrowding\"].apply(risk_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a clean output table\n",
    "prediction_table = future_panel[[\n",
    "    \"route_long_name\",      # 👈 full real route name\n",
    "    \"time\",\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"prob_overcrowding\",\n",
    "    \"Risk Level\"\n",
    "]].copy()\n",
    "\n",
    "prediction_table[\"prob_overcrowding\"] = prediction_table[\"prob_overcrowding\"].round(2)\n",
    "\n",
    "prediction_table.sort_values(\n",
    "    [\"time\",\"prob_overcrowding\"],\n",
    "    ascending=[True, False],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "prediction_table.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment**\n",
    "\n",
    "Step 1 — Export Your Prediction Table From Python\n",
    "\n",
    "You already have:\n",
    "\n",
    "prediction_table\n",
    "\n",
    "Let’s make sure it contains:\n",
    "\n",
    "Route Number\n",
    "\n",
    "Route Name\n",
    "\n",
    "Time\n",
    "\n",
    "Temperature\n",
    "\n",
    "Rain\n",
    "\n",
    "Probability\n",
    "\n",
    "Risk Level\n",
    "\n",
    "Run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_table_export = future_panel[[\n",
    "    \"route_short_name\",\n",
    "    \"route_long_name\",\n",
    "    \"time\",\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"prob_overcrowding\",\n",
    "    \"Risk Level\"\n",
    "]].copy()\n",
    "\n",
    "prediction_table_export.rename(columns={\n",
    "    \"route_short_name\": \"Route Number\",\n",
    "    \"route_long_name\": \"Route Name\",\n",
    "    \"time\": \"DateTime\",\n",
    "    \"temperature\": \"Temperature\",\n",
    "    \"precipitation\": \"Rain\",\n",
    "    \"prob_overcrowding\": \"Overcrowding Probability\"\n",
    "}, inplace=True)\n",
    "\n",
    "prediction_table_export[\"Overcrowding Probability\"] = prediction_table_export[\"Overcrowding Probability\"].round(3)\n",
    "\n",
    "prediction_table_export.to_csv(\"route_overcrowding_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
